import argparse
import json
import os
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta, timezone
from threading import Semaphore
from typing import Any, Optional

import arxiv
import requests

try:
    from dotenv import load_dotenv
except ImportError:  # pragma: no cover
    load_dotenv = None

if load_dotenv is not None:
    load_dotenv()

try:
    from jinja2 import Environment, StrictUndefined
except ImportError:  # pragma: no cover
    Environment = None
    StrictUndefined = None

PWC_BASE_URL = "https://arxiv.paperswithcode.com/api/v0/papers/"

# ============ é…ç½®å¸¸é‡ï¼ˆä¸éšç§çš„é…ç½®ç›´æ¥å†™æ­»ï¼‰ ============

# ArXiv æŸ¥è¯¢å…³é”®è¯
DEFAULT_ARXIV_QUERY = 'abs:"LLM safety" OR abs:"agent safety" OR abs:"AI agent" OR abs:"language model safety" OR abs:"autonomous agent"'

# æ¯æ¬¡è·å–è®ºæ–‡æ•°é‡ï¼ˆä¼šè·å–æ›´å¤šè®ºæ–‡ï¼Œç„¶åæŒ‰è¯„åˆ†ç­›é€‰ï¼‰
DEFAULT_MAX_RESULTS = 50  # è·å– 50 ç¯‡ï¼Œç­›é€‰å‡ºè¯„åˆ† >= 3 çš„å‰ 20 ç¯‡

# æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰ï¼š0 è¡¨ç¤ºä¸é™åˆ¶
DEFAULT_SINCE_HOURS = 0.0

# æœ€ä½è¯„åˆ†é˜ˆå€¼ï¼ˆä½äºæ­¤åˆ†æ•°çš„è®ºæ–‡ä¸æ¨é€ï¼‰
MIN_SCORE_THRESHOLD = 3.0

# æœ€ç»ˆæ¨é€è®ºæ–‡æ•°é‡
FINAL_PUSH_COUNT = 20

# Prompt æ¨¡æ¿æ–‡ä»¶
DEFAULT_PROMPT_FILE = "prompts/deepseek_summary_prompt.zh.j2"

# æ¨¡å‹é…ç½®
DEFAULT_MODEL = "glm-4.7"
DEFAULT_API_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"

# é»˜è®¤ max_tokensï¼ˆGLM-4.7 éœ€è¦æ›´å¤š tokens ç”¨äºæ¨ç†ï¼‰
DEFAULT_MAX_TOKENS = 2000

# å¹¶å‘å¤„ç†çº¿ç¨‹æ•°ï¼ˆä¸¥æ ¼æ§åˆ¶åŒæ—¶è¿›è¡Œçš„ API è¯·æ±‚æ•°ï¼‰
MAX_WORKERS = 3

# é‡è¯•é…ç½®
MAX_RETRIES = 3
RETRY_DELAY = 2  # ç§’

# è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰- é¿å…ç¬é—´å‘é€è¿‡å¤šè¯·æ±‚
REQUEST_INTERVAL = 0.5

# ArXiv API é…ç½®
ARXIV_PAGE_SIZE = 50  # æ¯æ¬¡è¯·æ±‚çš„ç»“æœæ•°é‡
ARXIV_DELAY_SECONDS = 3.0  # è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰
ARXIV_NUM_RETRIES = 5  # é‡è¯•æ¬¡æ•°

# é¢„ç¼–è¯‘æ­£åˆ™ï¼šåŒ¹é…ã€ç›¸å…³æ€§ã€‘X/5 æ ¼å¼çš„è¯„åˆ†
_SCORE_RE = re.compile(r'ã€ç›¸å…³æ€§ã€‘\s*(\d+(?:\.\d+)?)\s*/\s*5')

# é¢„ç¼–è¯‘æ­£åˆ™ï¼šåŒ¹é…ã€æ ‡ç­¾åã€‘å†…å®¹ æ ¼å¼çš„åˆ†ææ®µè½
_SECTION_RE = re.compile(r'ã€([^ã€‘]+)ã€‘\s*(.*?)(?=ã€|$)', re.DOTALL)


def _strtobool(v: Optional[str]) -> bool:
    if v is None:
        return False
    return v.strip().lower() in {"1", "true", "t", "yes", "y", "on"}


def _getenv_str(name: str, default: Optional[str] = None) -> Optional[str]:
    raw = os.getenv(name)
    if raw is None:
        return default
    raw = raw.strip()
    if raw == "":
        return default
    return raw


def _getenv_int(name: str, default: int) -> int:
    raw = os.getenv(name)
    if raw is None or raw.strip() == "":
        return default
    return int(raw)


def _getenv_float(name: str, default: float) -> float:
    raw = os.getenv(name)
    if raw is None or raw.strip() == "":
        return default
    return float(raw)


def _resolve_path(path: str) -> str:
    path = os.path.expanduser(path)
    if os.path.isabs(path):
        return path
    base_dir = os.path.dirname(os.path.abspath(__file__))
    return os.path.join(base_dir, path)


def _read_text_file(path: str) -> str:
    resolved = _resolve_path(path)
    with open(resolved, "r", encoding="utf-8") as f:
        return f.read()


def _compile_prompt_template(template_text: str):
    if Environment is None or StrictUndefined is None:  # pragma: no cover
        raise RuntimeError("ç¼ºå°‘ä¾èµ– Jinja2ï¼šè¯·å…ˆå®‰è£… `pip install Jinja2`")
    env = Environment(undefined=StrictUndefined, autoescape=False)
    return env.from_string(template_text)


def get_code_link(arxiv_url: str, session: requests.Session, timeout_s: int = 10) -> Optional[str]:
    """ä» PapersWithCode è·å–ä»£ç é“¾æ¥ï¼ˆè‹¥æœ‰ official repoï¼‰ã€‚"""
    arxiv_id = arxiv_url.rstrip("/").split("/")[-1].split("v")[0]
    try:
        resp = session.get(f"{PWC_BASE_URL}{arxiv_id}", timeout=timeout_s)
        if resp.status_code != 200:
            return None
        data = resp.json()
        official = data.get("official")
        if isinstance(official, dict):
            url = official.get("url")
            if isinstance(url, str) and url.startswith(("http://", "https://")):
                return url
    except requests.RequestException:
        return None
    except ValueError:
        return None
    return None


def _extract_score(analysis: str) -> float:
    """ä»åˆ†ææ–‡æœ¬ä¸­æå–ç›¸å…³æ€§è¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰ï¼Œé»˜è®¤è¿”å› 3.0"""
    # åŒ¹é…ã€ç›¸å…³æ€§ã€‘X/5 æ ¼å¼
    match = re.search(r'ã€ç›¸å…³æ€§ã€‘\s*(\d+(?:\.\d+)?)\s*/\s*5', analysis)
    if match:
        try:
            score = float(match.group(1))
            return max(1.0, min(5.0, score))  # é™åˆ¶åœ¨ 1-5 èŒƒå›´å†…
        except ValueError:
            pass
    return 3.0  # é»˜è®¤ä¸­ç­‰è¯„åˆ†


def _process_single_paper(
    res,
    index: int,
    total: int,
    *,
    session: requests.Session,
    skip_llm: bool,
    prompt_template,
    api_key: str,
    api_url: str,
    model: str,
    max_tokens: int,
    semaphore: Semaphore,
) -> dict:
    """å¤„ç†å•ç¯‡è®ºæ–‡ï¼ˆç”¨äºå¹¶å‘ï¼‰"""
    print(f"æ­£åœ¨åˆ†æç¬¬ {index}/{total} ç¯‡: {res.title}")

    code_url = get_code_link(res.entry_id, session=session)
    paper_info = {
        "title": res.title.strip(),
        "summary": (res.summary or "").replace("\n", " ").strip(),
        "url": res.entry_id,
    }

    if skip_llm:
        analysis = f"ã€æ‘˜è¦ï¼ˆæœªè°ƒç”¨ LLMï¼‰ã€‘\n{paper_info['summary']}\n"
        score = 3.0
    else:
        try:
            # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘ API è¯·æ±‚æ•°
            with semaphore:
                # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…ç¬é—´å‘é€è¿‡å¤šè¯·æ±‚
                time.sleep(REQUEST_INTERVAL)
                analysis = summarize_with_deepseek(
                    paper_info,
                    prompt_template=prompt_template,
                    api_key=api_key,
                    api_url=api_url,
                    model=model,
                    max_tokens=max_tokens,
                    session=session,
                )
            # æ‰“å° LLM è¿”å›çš„åŸå§‹å†…å®¹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            print(f"\n=== è®ºæ–‡ {index} LLM è¿”å›å†…å®¹ ===")
            print(analysis)
            print("===================\n")
            score = _extract_score(analysis)
        except Exception as e:
            print(f"è®ºæ–‡ {index} LLM è°ƒç”¨å¤±è´¥: {str(e)}")
            analysis = f"ã€LLM è§£æå¤±è´¥ã€‘{str(e)}\n\nã€æ‘˜è¦ã€‘{paper_info['summary']}"
            score = 3.0

    return {
        "title": paper_info["title"],
        "url": paper_info["url"],
        "code_url": code_url,
        "analysis": analysis,
        "score": score,
    }


def summarize_with_deepseek(
    paper: dict[str, str],
    *,
    prompt_template,
    api_key: str,
    api_url: str,
    model: str,
    max_tokens: int,
    session: requests.Session,
    timeout_s: int = 120,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 120 ç§’
) -> str:
    """ä½¿ç”¨ DeepSeekï¼ˆOpenAI Chat Completions å…¼å®¹ï¼‰è¿›è¡Œè®ºæ–‡æ·±åº¦æ€»ç»“ã€‚"""
    prompt_text = prompt_template.render(**paper).strip()

    payload: dict[str, Any] = {
        "model": model,
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„å­¦æœ¯è®ºæ–‡ç­›é€‰åŠ©æ‰‹ï¼Œä¸“æ³¨äº LLM Safetyã€Agent Safety å’Œ AI Agent é¢†åŸŸã€‚è¯·å®¢è§‚è¯„ä¼°è®ºæ–‡çš„ç›¸å…³æ€§å’Œåˆ›æ–°æ€§ã€‚",
            },
            {"role": "user", "content": prompt_text},
        ],
        "temperature": 1.0,
        "stream": False,
        "thinking": {
            "type": "disabled"
        },
        "max_tokens": max_tokens,
    }

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }

    # é‡è¯•é€»è¾‘
    for attempt in range(MAX_RETRIES):
        try:
            resp = session.post(api_url, headers=headers, json=payload, timeout=timeout_s)
            resp.raise_for_status()
            res_json = resp.json()

            if isinstance(res_json, dict) and "error" in res_json:
                err = res_json.get("error") or {}
                message = err.get("message") if isinstance(err, dict) else None
                raise RuntimeError(f"DeepSeek API æŠ¥é”™: {message or json.dumps(res_json, ensure_ascii=False)}")

            choices = res_json.get("choices") if isinstance(res_json, dict) else None
            if not isinstance(choices, list) or not choices:
                raise RuntimeError(f"API æœªé¢„æœŸå“åº”: {json.dumps(res_json, ensure_ascii=False)}")

            message = choices[0].get("message") if isinstance(choices[0], dict) else None

            # GLM-4.7 ç­‰æ¨ç†æ¨¡å‹å¯èƒ½å°†å†…å®¹æ”¾åœ¨ reasoning_content ä¸­
            content = None
            if isinstance(message, dict):
                content = message.get("content")
                # å¦‚æœ content ä¸ºç©ºï¼Œå°è¯•ä» reasoning_content ä¸­æå–
                if not content or not content.strip():
                    reasoning_content = message.get("reasoning_content")
                    if reasoning_content:
                        print("è­¦å‘Šï¼šæ¨¡å‹è¿”å›çš„ content ä¸ºç©ºï¼Œä½¿ç”¨ reasoning_content")
                        content = reasoning_content

            if not isinstance(content, str) or not content.strip():
                raise RuntimeError(f"API æœªè¿”å› content: {json.dumps(res_json, ensure_ascii=False)}")

            return content.strip()

        except (requests.exceptions.Timeout, requests.exceptions.HTTPError, Exception) as e:
            is_last_attempt = attempt == MAX_RETRIES - 1

            # 429 é”™è¯¯ä½¿ç”¨æŒ‡æ•°é€€é¿ï¼Œå…¶ä»–é”™è¯¯ä½¿ç”¨çº¿æ€§é€€é¿
            if isinstance(e, requests.exceptions.HTTPError) and e.response.status_code == 429:
                wait_time = RETRY_DELAY * (attempt + 2) * 2
                error_msg = "API é™æµ"
            else:
                wait_time = RETRY_DELAY * (attempt + 1)
                error_msg = "è¯·æ±‚è¶…æ—¶" if isinstance(e, requests.exceptions.Timeout) else f"è¯·æ±‚å¤±è´¥: {str(e)}"

            if not is_last_attempt:
                print(f"{error_msg}ï¼Œ{wait_time} ç§’åé‡è¯•ï¼ˆç¬¬ {attempt + 1}/{MAX_RETRIES} æ¬¡ï¼‰...")
                time.sleep(wait_time)
            else:
                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                raise


def _extract_section(analysis: str, section_name: str) -> Optional[str]:
    """ä»åˆ†ææ–‡æœ¬ä¸­æå–æŒ‡å®šæ®µè½çš„å†…å®¹"""
    match = re.search(rf'ã€{re.escape(section_name)}ã€‘\s*(.*?)(?=ã€|$)', analysis, re.DOTALL)
    return match.group(1).strip() if match else None


def _feishu_card_payload(title: str, papers: list[dict], footer_note: str) -> dict[str, Any]:
    """ç”Ÿæˆé£ä¹¦å¯Œæ–‡æœ¬å¡ç‰‡ payloadï¼ˆæ”¯æŒå¤šç¯‡è®ºæ–‡ï¼‰"""
    elements = []

    for i, paper in enumerate(papers):
        analysis = paper['analysis']

        # æå–è¯„åˆ†
        score_match = re.search(r'ã€ç›¸å…³æ€§ã€‘\s*(\d+(?:\.\d+)?)\s*/\s*5', analysis)
        score_text = f"<font color='red'>({score_match.group(1)}/5)</font>" if score_match else ""

        # æ ‡é¢˜ï¼ˆå¸¦è¯„åˆ†ï¼‰
        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"**{i+1}/{len(papers)}. <font color='blue'>{paper['title']}</font>** {score_text}"
            }
        })

        # é“¾æ¥æŒ‰é’®
        actions = [{
            "tag": "button",
            "text": {"tag": "plain_text", "content": "æŸ¥çœ‹è®ºæ–‡"},
            "type": "primary",
            "url": paper['url']
        }]
        if paper.get('code_url'):
            actions.append({
                "tag": "button",
                "text": {"tag": "plain_text", "content": "æŸ¥çœ‹ä»£ç "},
                "type": "default",
                "url": paper['code_url']
            })
        elements.append({"tag": "action", "actions": actions})

        # åˆå¹¶ï¼šé—®é¢˜å®šä¹‰ + æ–¹æ³•æ ¸å¿ƒ + ä¸»è¦å‘ç°
        core_sections = [
            ("é—®é¢˜å®šä¹‰", "violet"),
            ("æ–¹æ³•æ ¸å¿ƒ", "blue"),
            ("ä¸»è¦å‘ç°", "violet"),
        ]
        core_content = []
        for section_name, color in core_sections:
            content = _extract_section(analysis, section_name)
            if content:
                core_content.append(f"<font color='{color}'>**ã€{section_name}ã€‘**</font>\n{content}")

        if core_content:
            elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": "\n".join(core_content)
                }
            })

        # åˆå¹¶ï¼šå±€é™æ€§æ¨æµ‹ + æ½œåœ¨å…³è”
        analysis_sections = [
            ("å±€é™æ€§æ¨æµ‹", "orange"),
            ("æ½œåœ¨å…³è”", "green"),
        ]
        analysis_content = []
        for section_name, color in analysis_sections:
            content = _extract_section(analysis, section_name)
            if content:
                analysis_content.append(f"<font color='{color}'>**ã€{section_name}ã€‘**</font>\n{content}")

        if analysis_content:
            elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": "\n".join(analysis_content)
                }
            })

        # æå–ä¸€å¥è¯ç»“è®º
        conclusion = _extract_section(analysis, "ä¸€å¥è¯ç»“è®º")
        if conclusion:
            elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"<font color='blue'>**ã€ä¸€å¥è¯ç»“è®ºã€‘**</font>\n{conclusion}"
                }
            })

        # å¦‚æœä¸æ˜¯æœ€åä¸€ç¯‡ï¼Œæ·»åŠ åˆ†éš”çº¿
        if i < len(papers) - 1:
            elements.append({"tag": "hr"})

    # æ·»åŠ é¡µè„š
    elements.append({"tag": "hr"})
    elements.append({
        "tag": "note",
        "elements": [{"tag": "plain_text", "content": footer_note}]
    })

    return {
        "msg_type": "interactive",
        "card": {
            "header": {
                "title": {"tag": "plain_text", "content": title},
                "template": "blue"
            },
            "elements": elements
        }
    }


def push_to_feishu(
    papers: list[dict],
    *,
    webhook: str,
    session: requests.Session,
    title: str,
    footer_note: str,
    timeout_s: int = 15,
) -> None:
    """å‘é€é£ä¹¦å¯Œæ–‡æœ¬å¡ç‰‡ï¼ˆå¤±è´¥ä¼šæŠ›å¼‚å¸¸ï¼‰ã€‚

    Args:
        papers: è®ºæ–‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« title, url, code_url, analysis
        webhook: é£ä¹¦ Webhook åœ°å€
        session: requests.Session å¯¹è±¡
        title: å¡ç‰‡æ ‡é¢˜
        footer_note: é¡µè„šæ–‡æœ¬
        timeout_s: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    """
    headers = {"Content-Type": "application/json"}
    payload = _feishu_card_payload(title=title, papers=papers, footer_note=footer_note)
    resp = session.post(webhook, headers=headers, json=payload, timeout=timeout_s)
    resp.raise_for_status()
    data = resp.json()
    if isinstance(data, dict):
        code = data.get("code", data.get("StatusCode"))
        if code not in (0, "0", None):
            raise RuntimeError(f"é£ä¹¦è¿”å›é”™è¯¯: {json.dumps(data, ensure_ascii=False)}")


def _write_github_step_summary(markdown: str) -> None:
    path = os.getenv("GITHUB_STEP_SUMMARY")
    if not path:
        return
    try:
        with open(path, "a", encoding="utf-8") as f:
            f.write(markdown.rstrip() + "\n")
    except OSError:
        return


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Fetch latest arXiv papers, summarize, and push to Feishu.")
    parser.add_argument("--query", default=_getenv_str("ARXIV_QUERY", DEFAULT_ARXIV_QUERY))
    parser.add_argument("--max-results", type=int, default=_getenv_int("MAX_RESULTS", DEFAULT_MAX_RESULTS))
    parser.add_argument("--since-hours", type=float, default=_getenv_float("SINCE_HOURS", DEFAULT_SINCE_HOURS))

    parser.add_argument("--feishu-webhook", default=_getenv_str("FEISHU_WEBHOOK"))
    parser.add_argument("--per-paper", action="store_true", default=_strtobool(os.getenv("FEISHU_PER_PAPER")))

    parser.add_argument("--deepseek-api-key", default=_getenv_str("DEEPSEEK_API_KEY"))
    parser.add_argument("--deepseek-model", default=_getenv_str("DEEPSEEK_MODEL", DEFAULT_MODEL))
    parser.add_argument("--deepseek-api-url", default=_getenv_str("DEEPSEEK_API_URL"))  # å¦‚æœæœªæŒ‡å®šï¼Œå°†æ ¹æ®æ¨¡å‹è‡ªåŠ¨é€‰æ‹©
    parser.add_argument("--deepseek-max-tokens", type=int, default=_getenv_int("DEEPSEEK_MAX_TOKENS", DEFAULT_MAX_TOKENS))
    parser.add_argument("--skip-llm", action="store_true", default=_strtobool(os.getenv("SKIP_LLM")))
    parser.add_argument("--prompt-file", default=_getenv_str("PROMPT_FILE", DEFAULT_PROMPT_FILE))

    parser.add_argument("--dry-run", action="store_true", default=_strtobool(os.getenv("DRY_RUN")))
    return parser.parse_args()


def main() -> int:
    args = _parse_args()
    session = requests.Session()

    # å¦‚æœæœªæŒ‡å®š API URLï¼Œä½¿ç”¨é»˜è®¤å€¼
    if not args.deepseek_api_url:
        args.deepseek_api_url = DEFAULT_API_URL
        print(f"ä½¿ç”¨é»˜è®¤ API ç«¯ç‚¹ï¼š{args.deepseek_api_url}ï¼ˆæ¨¡å‹ï¼š{args.deepseek_model}ï¼‰")

    if not args.dry_run and not args.feishu_webhook:
        print("ç¼ºå°‘ FEISHU_WEBHOOKï¼šè¯·åœ¨ç¯å¢ƒå˜é‡æˆ–å‚æ•°ä¸­è®¾ç½® --feishu-webhookã€‚", file=sys.stderr)
        return 2
    if not args.skip_llm and not args.deepseek_api_key:
        print("ç¼ºå°‘ DEEPSEEK_API_KEYï¼šè¯·åœ¨ç¯å¢ƒå˜é‡æˆ–å‚æ•°ä¸­è®¾ç½® --deepseek-api-keyï¼Œæˆ–ä½¿ç”¨ --skip-llmã€‚", file=sys.stderr)
        return 2

    prompt_template = None
    if not args.skip_llm:
        try:
            template_text = _read_text_file(args.prompt_file)
            prompt_template = _compile_prompt_template(template_text)
        except Exception as e:
            print(f"æ— æ³•åŠ è½½ Prompt æ¨¡æ¿ï¼ˆ{args.prompt_file}ï¼‰ï¼š{e}", file=sys.stderr)
            return 2

    print("æ­£åœ¨æœé›†æœ€æ–°è®ºæ–‡...")
    # é…ç½® ArXiv å®¢æˆ·ç«¯ï¼Œæ·»åŠ é€Ÿç‡é™åˆ¶å’Œé‡è¯•æœºåˆ¶
    client = arxiv.Client(
        page_size=ARXIV_PAGE_SIZE,
        delay_seconds=ARXIV_DELAY_SECONDS,
        num_retries=ARXIV_NUM_RETRIES
    )
    search = arxiv.Search(
        query=args.query,
        max_results=args.max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate,
    )

    results = list(client.results(search))
    if args.since_hours > 0:
        now = datetime.now(timezone.utc)
        threshold = now - timedelta(hours=float(args.since_hours))
        results = [r for r in results if getattr(r, "published", None) and r.published.replace(tzinfo=timezone.utc) >= threshold]

    if not results:
        msg = "ä»Šæ—¥æš‚æ— æ–°è®ºæ–‡ã€‚"
        print(msg)
        _write_github_step_summary(f"## ArXiv æ¯æ—¥æ¨é€\n\n{msg}\n")
        if not args.dry_run and args.feishu_webhook:
            push_to_feishu(
                msg,
                webhook=args.feishu_webhook,
                session=session,
                title=f"ğŸš€ ArXiv {datetime.now().strftime('%m-%d')}",
                footer_note="è‡ªåŠ¨ç”Ÿæˆï¼šæ— æ–°è®ºæ–‡",
            )
        return 0

    # ç¬¬ä¸€æ­¥ï¼šå¹¶å‘åˆ†ææ‰€æœ‰è®ºæ–‡å¹¶æå–è¯„åˆ†
    paper_data: list[dict] = []
    total = len(results)

    print(f"å¼€å§‹å¹¶å‘åˆ†æ {total} ç¯‡è®ºæ–‡ï¼ˆå¹¶å‘æ•°ï¼š{MAX_WORKERS}ï¼‰...")

    # åˆ›å»ºä¿¡å·é‡ï¼Œä¸¥æ ¼æ§åˆ¶åŒæ—¶è¿›è¡Œçš„ API è¯·æ±‚æ•°
    api_semaphore = Semaphore(MAX_WORKERS)

    # ThreadPoolExecutor çš„ max_workers è®¾ä¸º MAX_WORKERS å³å¯
    # Semaphore å·²ç»æ§åˆ¶äº†å¹¶å‘ API è¯·æ±‚æ•°ï¼Œä¸éœ€è¦é¢å¤–çš„çº¿ç¨‹
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_index = {
            executor.submit(
                _process_single_paper,
                res,
                i,
                total,
                session=session,
                skip_llm=args.skip_llm,
                prompt_template=prompt_template,
                api_key=args.deepseek_api_key,
                api_url=args.deepseek_api_url,
                model=args.deepseek_model,
                max_tokens=args.deepseek_max_tokens,
                semaphore=api_semaphore,
            ): i
            for i, res in enumerate(results, start=1)
        }

        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_index):
            try:
                paper = future.result()
                paper_data.append(paper)
            except Exception as e:
                index = future_to_index[future]
                print(f"å¤„ç†è®ºæ–‡ {index} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                # ç»§ç»­å¤„ç†å…¶ä»–è®ºæ–‡

    # ç¬¬äºŒæ­¥ï¼šæŒ‰è¯„åˆ†ä»é«˜åˆ°ä½æ’åº
    paper_data.sort(key=lambda x: x["score"], reverse=True)

    # ç¬¬ä¸‰æ­¥ï¼šè¿‡æ»¤ä½åˆ†è®ºæ–‡ï¼ˆè¯„åˆ† < MIN_SCORE_THRESHOLD çš„ä¸æ¨é€ï¼‰
    filtered_papers = [p for p in paper_data if p["score"] >= MIN_SCORE_THRESHOLD]

    if not filtered_papers:
        msg = f"ä»Šæ—¥æ— é«˜ç›¸å…³æ€§è®ºæ–‡ï¼ˆæ‰€æœ‰è®ºæ–‡è¯„åˆ† < {MIN_SCORE_THRESHOLD}ï¼‰ã€‚"
        print(msg)
        _write_github_step_summary(f"## ArXiv æ¯æ—¥æ¨é€\n\n{msg}\n")
        # ä¸æ¨é€ç©ºæ¶ˆæ¯åˆ°é£ä¹¦
        return 0

    # ç¬¬å››æ­¥ï¼šåªä¿ç•™å‰ FINAL_PUSH_COUNT ç¯‡è®ºæ–‡
    final_papers = filtered_papers[:FINAL_PUSH_COUNT]
    print(f"ç­›é€‰åå…± {len(filtered_papers)} ç¯‡é«˜åˆ†è®ºæ–‡ï¼Œæ¨é€å‰ {len(final_papers)} ç¯‡")

    # ç¬¬äº”æ­¥ï¼šç”Ÿæˆæ¨é€å†…å®¹
    date_label = datetime.now().strftime("%m-%d")
    card_title = f"ğŸš€ ArXiv {date_label}"
    footer_note = f"è‡ªåŠ¨ç”Ÿæˆ | å…± {len(final_papers)} ç¯‡é«˜ç›¸å…³æ€§è®ºæ–‡"

    # ç”Ÿæˆ GitHub Step Summary
    summary_blocks = []
    for i, paper in enumerate(final_papers, start=1):
        code_md = f" | [ğŸ’» ä»£ç ]({paper['code_url']})" if paper.get('code_url') else ""
        header = f"### {i}/{len(final_papers)}. {paper['title']}\nğŸ”— [åŸæ–‡]({paper['url']}){code_md}\n"
        summary_blocks.append(header + paper['analysis'].strip() + "\n")

    summary_md = f"## ArXiv æ¯æ—¥æ¨é€ ({date_label})\n\n" + "\n---\n\n".join(summary_blocks)
    _write_github_step_summary(summary_md)

    if args.dry_run:
        print(summary_md)
        return 0

    # æ¨é€åˆ°é£ä¹¦
    if not args.feishu_webhook:
        print("æœªé…ç½®é£ä¹¦ Webhookï¼Œè·³è¿‡æ¨é€")
        return 0

    if args.per_paper:
        # æ¯ç¯‡è®ºæ–‡å•ç‹¬æ¨é€
        for i, paper in enumerate(final_papers, start=1):
            push_to_feishu(
                [paper],
                webhook=args.feishu_webhook,
                session=session,
                title=f"ğŸš€ ArXiv {date_label} ({i}/{len(final_papers)})",
                footer_note=footer_note,
            )
    else:
        # åˆå¹¶æ¨é€
        push_to_feishu(
            final_papers,
            webhook=args.feishu_webhook,
            session=session,
            title=card_title,
            footer_note=footer_note,
        )

    print("æ¨é€æˆåŠŸï¼")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
