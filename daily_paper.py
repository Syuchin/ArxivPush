import argparse
import json
import os
import re
import sys
from datetime import datetime, timedelta, timezone
from typing import Any, Optional

import arxiv
import requests

try:
    from dotenv import load_dotenv
except ImportError:  # pragma: no cover
    load_dotenv = None

if load_dotenv is not None:
    load_dotenv()

try:
    from jinja2 import Environment, StrictUndefined
except ImportError:  # pragma: no cover
    Environment = None
    StrictUndefined = None

PWC_BASE_URL = "https://arxiv.paperswithcode.com/api/v0/papers/"

# ============ é…ç½®å¸¸é‡ï¼ˆä¸éšç§çš„é…ç½®ç›´æ¥å†™æ­»ï¼‰ ============

# ArXiv æŸ¥è¯¢å…³é”®è¯
DEFAULT_ARXIV_QUERY = 'abs:"LLM safety" OR abs:"agent safety" OR abs:"AI agent" OR abs:"language model safety" OR abs:"autonomous agent"'

# æ¯æ¬¡è·å–è®ºæ–‡æ•°é‡ï¼ˆä¼šè·å–æ›´å¤šè®ºæ–‡ï¼Œç„¶åæŒ‰è¯„åˆ†ç­›é€‰ï¼‰
DEFAULT_MAX_RESULTS = 50  # è·å– 50 ç¯‡ï¼Œç­›é€‰å‡ºè¯„åˆ† >= 3 çš„å‰ 20 ç¯‡

# æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰ï¼š0 è¡¨ç¤ºä¸é™åˆ¶
DEFAULT_SINCE_HOURS = 0.0

# æœ€ä½è¯„åˆ†é˜ˆå€¼ï¼ˆä½äºæ­¤åˆ†æ•°çš„è®ºæ–‡ä¸æ¨é€ï¼‰
MIN_SCORE_THRESHOLD = 3.0

# æœ€ç»ˆæ¨é€è®ºæ–‡æ•°é‡
FINAL_PUSH_COUNT = 20

# Prompt æ¨¡æ¿æ–‡ä»¶
DEFAULT_PROMPT_FILE = "prompts/deepseek_summary_prompt.zh.j2"

# æ¨¡å‹é…ç½®
DEFAULT_MODEL = "glm-4.7-flash"
DEFAULT_API_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"

# é»˜è®¤ max_tokensï¼ˆGLM-4.7 éœ€è¦æ›´å¤š tokens ç”¨äºæ¨ç†ï¼‰
DEFAULT_MAX_TOKENS = 2000


def _strtobool(v: Optional[str]) -> bool:
    if v is None:
        return False
    return v.strip().lower() in {"1", "true", "t", "yes", "y", "on"}


def _getenv_str(name: str, default: Optional[str] = None) -> Optional[str]:
    raw = os.getenv(name)
    if raw is None:
        return default
    raw = raw.strip()
    if raw == "":
        return default
    return raw


def _getenv_int(name: str, default: int) -> int:
    raw = os.getenv(name)
    if raw is None or raw.strip() == "":
        return default
    return int(raw)


def _getenv_float(name: str, default: float) -> float:
    raw = os.getenv(name)
    if raw is None or raw.strip() == "":
        return default
    return float(raw)


def _resolve_path(path: str) -> str:
    path = os.path.expanduser(path)
    if os.path.isabs(path):
        return path
    base_dir = os.path.dirname(os.path.abspath(__file__))
    return os.path.join(base_dir, path)


def _read_text_file(path: str) -> str:
    resolved = _resolve_path(path)
    with open(resolved, "r", encoding="utf-8") as f:
        return f.read()


def _compile_prompt_template(template_text: str):
    if Environment is None or StrictUndefined is None:  # pragma: no cover
        raise RuntimeError("ç¼ºå°‘ä¾èµ– Jinja2ï¼šè¯·å…ˆå®‰è£… `pip install Jinja2`")
    env = Environment(undefined=StrictUndefined, autoescape=False)
    return env.from_string(template_text)


def get_code_link(arxiv_url: str, session: requests.Session, timeout_s: int = 10) -> Optional[str]:
    """ä» PapersWithCode è·å–ä»£ç é“¾æ¥ï¼ˆè‹¥æœ‰ official repoï¼‰ã€‚"""
    arxiv_id = arxiv_url.rstrip("/").split("/")[-1].split("v")[0]
    try:
        resp = session.get(f"{PWC_BASE_URL}{arxiv_id}", timeout=timeout_s)
        if resp.status_code != 200:
            return None
        data = resp.json()
        official = data.get("official")
        if isinstance(official, dict):
            url = official.get("url")
            if isinstance(url, str) and url.startswith(("http://", "https://")):
                return url
    except requests.RequestException:
        return None
    except ValueError:
        return None
    return None


def _extract_score(analysis: str) -> float:
    """ä»åˆ†ææ–‡æœ¬ä¸­æå–ç›¸å…³æ€§è¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰ï¼Œé»˜è®¤è¿”å› 3.0"""
    match = re.search(r'è¯„åˆ†[ï¼š:]\s*(\d+(?:\.\d+)?)\s*/\s*5', analysis)
    if match:
        try:
            score = float(match.group(1))
            return max(1.0, min(5.0, score))  # é™åˆ¶åœ¨ 1-5 èŒƒå›´å†…
        except ValueError:
            pass
    return 3.0  # é»˜è®¤ä¸­ç­‰è¯„åˆ†


def summarize_with_deepseek(
    paper: dict[str, str],
    *,
    prompt_template,
    api_key: str,
    api_url: str,
    model: str,
    max_tokens: int,
    session: requests.Session,
    timeout_s: int = 60,
) -> str:
    """ä½¿ç”¨ DeepSeekï¼ˆOpenAI Chat Completions å…¼å®¹ï¼‰è¿›è¡Œè®ºæ–‡æ·±åº¦æ€»ç»“ã€‚"""
    prompt_text = prompt_template.render(**paper).strip()

    payload: dict[str, Any] = {
        "model": model,
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„å­¦æœ¯è®ºæ–‡ç­›é€‰åŠ©æ‰‹ï¼Œä¸“æ³¨äº LLM Safetyã€Agent Safety å’Œ AI Agent é¢†åŸŸã€‚è¯·å®¢è§‚è¯„ä¼°è®ºæ–‡çš„ç›¸å…³æ€§å’Œåˆ›æ–°æ€§ã€‚",
            },
            {"role": "user", "content": prompt_text},
        ],
        "temperature": 0.2,
        "stream": False,
        "max_tokens": max_tokens,
    }

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }

    resp = session.post(api_url, headers=headers, json=payload, timeout=timeout_s)
    resp.raise_for_status()
    res_json = resp.json()

    if isinstance(res_json, dict) and "error" in res_json:
        err = res_json.get("error") or {}
        message = err.get("message") if isinstance(err, dict) else None
        raise RuntimeError(f"DeepSeek API æŠ¥é”™: {message or json.dumps(res_json, ensure_ascii=False)}")

    choices = res_json.get("choices") if isinstance(res_json, dict) else None
    if not isinstance(choices, list) or not choices:
        raise RuntimeError(f"API æœªé¢„æœŸå“åº”: {json.dumps(res_json, ensure_ascii=False)}")

    message = choices[0].get("message") if isinstance(choices[0], dict) else None

    # GLM-4.7 ç­‰æ¨ç†æ¨¡å‹å¯èƒ½å°†å†…å®¹æ”¾åœ¨ reasoning_content ä¸­
    content = None
    if isinstance(message, dict):
        content = message.get("content")
        # å¦‚æœ content ä¸ºç©ºï¼Œå°è¯•ä» reasoning_content ä¸­æå–
        if not content or not content.strip():
            reasoning_content = message.get("reasoning_content")
            if reasoning_content:
                print("è­¦å‘Šï¼šæ¨¡å‹è¿”å›çš„ content ä¸ºç©ºï¼Œä½¿ç”¨ reasoning_content")
                content = reasoning_content

    if not isinstance(content, str) or not content.strip():
        raise RuntimeError(f"API æœªè¿”å› content: {json.dumps(res_json, ensure_ascii=False)}")
    return content.strip()


def _feishu_card_payload(title: str, papers: list[dict], footer_note: str) -> dict[str, Any]:
    """ç”Ÿæˆé£ä¹¦å¯Œæ–‡æœ¬å¡ç‰‡ payloadï¼ˆæ”¯æŒå¤šç¯‡è®ºæ–‡ï¼‰"""
    elements = []

    for i, paper in enumerate(papers):
        # æå–è¯„åˆ†
        analysis = paper['analysis']
        score_match = re.search(r'ã€ç›¸å…³æ€§ã€‘\s*(\d+(?:\.\d+)?)\s*/\s*5', analysis)
        score_text = f"<font color='red'>({score_match.group(1)}/5)</font>" if score_match else ""

        # æ ‡é¢˜ï¼ˆå¸¦è¯„åˆ†ï¼‰
        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"**{i+1}/{len(papers)}. <font color='blue'>{paper['title']}</font>** {score_text}"
            }
        })

        # é“¾æ¥æŒ‰é’®
        actions = [{
            "tag": "button",
            "text": {"tag": "plain_text", "content": "æŸ¥çœ‹è®ºæ–‡"},
            "type": "primary",
            "url": paper['url']
        }]
        if paper.get('code_url'):
            actions.append({
                "tag": "button",
                "text": {"tag": "plain_text", "content": "æŸ¥çœ‹ä»£ç "},
                "type": "default",
                "url": paper['code_url']
            })
        elements.append({"tag": "action", "actions": actions})

        # åˆå¹¶ï¼šé—®é¢˜å®šä¹‰ + æ–¹æ³•æ ¸å¿ƒ + ä¸»è¦å‘ç°
        core_content = []

        problem_match = re.search(r'ã€é—®é¢˜å®šä¹‰ã€‘\s*(.*?)(?=ã€|$)', analysis, re.DOTALL)
        if problem_match:
            core_content.append(f"<font color='violet'>**ã€é—®é¢˜å®šä¹‰ã€‘**</font>\n{problem_match.group(1).strip()}")

        method_match = re.search(r'ã€æ–¹æ³•æ ¸å¿ƒã€‘\s*(.*?)(?=ã€|$)', analysis, re.DOTALL)
        if method_match:
            core_content.append(f"<font color='blue'>**ã€æ–¹æ³•æ ¸å¿ƒã€‘**</font>\n{method_match.group(1).strip()}")

        finding_match = re.search(r'ã€ä¸»è¦å‘ç°ã€‘\s*(.*?)(?=ã€|$)', analysis, re.DOTALL)
        if finding_match:
            core_content.append(f"<font color='violet'>**ã€ä¸»è¦å‘ç°ã€‘**</font>\n{finding_match.group(1).strip()}")

        if core_content:
            elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": "\n".join(core_content)
                }
            })

        # åˆå¹¶ï¼šå±€é™æ€§æ¨æµ‹ + æ½œåœ¨å…³è”
        analysis_content = []

        limitation_match = re.search(r'ã€å±€é™æ€§æ¨æµ‹ã€‘\s*(.*?)(?=ã€|$)', analysis, re.DOTALL)
        if limitation_match:
            analysis_content.append(f"<font color='orange'>**ã€å±€é™æ€§æ¨æµ‹ã€‘**</font>\n{limitation_match.group(1).strip()}")

        relation_match = re.search(r'ã€æ½œåœ¨å…³è”ã€‘\s*(.*?)(?=ã€|$)', analysis, re.DOTALL)
        if relation_match:
            analysis_content.append(f"<font color='green'>**ã€æ½œåœ¨å…³è”ã€‘**</font>\n{relation_match.group(1).strip()}")

        if analysis_content:
            elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": "\n".join(analysis_content)
                }
            })

        # æå–ä¸€å¥è¯ç»“è®º
        conclusion_match = re.search(r'ã€ä¸€å¥è¯ç»“è®ºã€‘\s*(.*?)(?=ã€|$)', analysis, re.DOTALL)
        if conclusion_match:
            conclusion_text = conclusion_match.group(1).strip()
            elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"<font color='blue'>**ã€ä¸€å¥è¯ç»“è®ºã€‘**</font>\n{conclusion_text}"
                }
            })

        # å¦‚æœä¸æ˜¯æœ€åä¸€ç¯‡ï¼Œæ·»åŠ åˆ†éš”çº¿
        if i < len(papers) - 1:
            elements.append({"tag": "hr"})

    # æ·»åŠ é¡µè„š
    elements.append({"tag": "hr"})
    elements.append({
        "tag": "note",
        "elements": [{"tag": "plain_text", "content": footer_note}]
    })

    return {
        "msg_type": "interactive",
        "card": {
            "header": {
                "title": {"tag": "plain_text", "content": title},
                "template": "blue"
            },
            "elements": elements
        }
    }


def push_to_feishu(
    papers: list[dict],
    *,
    webhook: str,
    session: requests.Session,
    title: str,
    footer_note: str,
    timeout_s: int = 15,
) -> None:
    """å‘é€é£ä¹¦å¯Œæ–‡æœ¬å¡ç‰‡ï¼ˆå¤±è´¥ä¼šæŠ›å¼‚å¸¸ï¼‰ã€‚

    Args:
        papers: è®ºæ–‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« title, url, code_url, analysis
        webhook: é£ä¹¦ Webhook åœ°å€
        session: requests.Session å¯¹è±¡
        title: å¡ç‰‡æ ‡é¢˜
        footer_note: é¡µè„šæ–‡æœ¬
        timeout_s: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    """
    headers = {"Content-Type": "application/json"}
    payload = _feishu_card_payload(title=title, papers=papers, footer_note=footer_note)
    resp = session.post(webhook, headers=headers, json=payload, timeout=timeout_s)
    resp.raise_for_status()
    data = resp.json()
    if isinstance(data, dict):
        code = data.get("code", data.get("StatusCode"))
        if code not in (0, "0", None):
            raise RuntimeError(f"é£ä¹¦è¿”å›é”™è¯¯: {json.dumps(data, ensure_ascii=False)}")


def _write_github_step_summary(markdown: str) -> None:
    path = os.getenv("GITHUB_STEP_SUMMARY")
    if not path:
        return
    try:
        with open(path, "a", encoding="utf-8") as f:
            f.write(markdown.rstrip() + "\n")
    except OSError:
        return


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Fetch latest arXiv papers, summarize, and push to Feishu.")
    parser.add_argument("--query", default=_getenv_str("ARXIV_QUERY", DEFAULT_ARXIV_QUERY))
    parser.add_argument("--max-results", type=int, default=_getenv_int("MAX_RESULTS", DEFAULT_MAX_RESULTS))
    parser.add_argument("--since-hours", type=float, default=_getenv_float("SINCE_HOURS", DEFAULT_SINCE_HOURS))

    parser.add_argument("--feishu-webhook", default=_getenv_str("FEISHU_WEBHOOK"))
    parser.add_argument("--per-paper", action="store_true", default=_strtobool(os.getenv("FEISHU_PER_PAPER")))

    parser.add_argument("--deepseek-api-key", default=_getenv_str("DEEPSEEK_API_KEY"))
    parser.add_argument("--deepseek-model", default=_getenv_str("DEEPSEEK_MODEL", DEFAULT_MODEL))
    parser.add_argument("--deepseek-api-url", default=_getenv_str("DEEPSEEK_API_URL"))  # å¦‚æœæœªæŒ‡å®šï¼Œå°†æ ¹æ®æ¨¡å‹è‡ªåŠ¨é€‰æ‹©
    parser.add_argument("--deepseek-max-tokens", type=int, default=_getenv_int("DEEPSEEK_MAX_TOKENS", DEFAULT_MAX_TOKENS))
    parser.add_argument("--skip-llm", action="store_true", default=_strtobool(os.getenv("SKIP_LLM")))
    parser.add_argument("--prompt-file", default=_getenv_str("PROMPT_FILE", DEFAULT_PROMPT_FILE))

    parser.add_argument("--dry-run", action="store_true", default=_strtobool(os.getenv("DRY_RUN")))
    return parser.parse_args()


def main() -> int:
    args = _parse_args()
    session = requests.Session()

    # å¦‚æœæœªæŒ‡å®š API URLï¼Œä½¿ç”¨é»˜è®¤å€¼
    if not args.deepseek_api_url:
        args.deepseek_api_url = DEFAULT_API_URL
        print(f"ä½¿ç”¨é»˜è®¤ API ç«¯ç‚¹ï¼š{args.deepseek_api_url}ï¼ˆæ¨¡å‹ï¼š{args.deepseek_model}ï¼‰")

    if not args.dry_run and not args.feishu_webhook:
        print("ç¼ºå°‘ FEISHU_WEBHOOKï¼šè¯·åœ¨ç¯å¢ƒå˜é‡æˆ–å‚æ•°ä¸­è®¾ç½® --feishu-webhookã€‚", file=sys.stderr)
        return 2
    if not args.skip_llm and not args.deepseek_api_key:
        print("ç¼ºå°‘ DEEPSEEK_API_KEYï¼šè¯·åœ¨ç¯å¢ƒå˜é‡æˆ–å‚æ•°ä¸­è®¾ç½® --deepseek-api-keyï¼Œæˆ–ä½¿ç”¨ --skip-llmã€‚", file=sys.stderr)
        return 2

    prompt_template = None
    if not args.skip_llm:
        try:
            template_text = _read_text_file(args.prompt_file)
            prompt_template = _compile_prompt_template(template_text)
        except Exception as e:
            print(f"æ— æ³•åŠ è½½ Prompt æ¨¡æ¿ï¼ˆ{args.prompt_file}ï¼‰ï¼š{e}", file=sys.stderr)
            return 2

    print("æ­£åœ¨æœé›†æœ€æ–°è®ºæ–‡...")
    client = arxiv.Client()
    search = arxiv.Search(
        query=args.query,
        max_results=args.max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate,
    )

    results = list(client.results(search))
    if args.since_hours > 0:
        now = datetime.now(timezone.utc)
        threshold = now - timedelta(hours=float(args.since_hours))
        results = [r for r in results if getattr(r, "published", None) and r.published.replace(tzinfo=timezone.utc) >= threshold]

    if not results:
        msg = "ä»Šæ—¥æš‚æ— æ–°è®ºæ–‡ã€‚"
        print(msg)
        _write_github_step_summary(f"## ArXiv æ¯æ—¥æ¨é€\n\n{msg}\n")
        if not args.dry_run and args.feishu_webhook:
            push_to_feishu(
                msg,
                webhook=args.feishu_webhook,
                session=session,
                title=f"ğŸš€ ArXiv {datetime.now().strftime('%m-%d')}",
                footer_note="è‡ªåŠ¨ç”Ÿæˆï¼šæ— æ–°è®ºæ–‡",
            )
        return 0

    # ç¬¬ä¸€æ­¥ï¼šåˆ†ææ‰€æœ‰è®ºæ–‡å¹¶æå–è¯„åˆ†
    paper_data: list[dict] = []
    total = len(results)
    for i, res in enumerate(results, start=1):
        print(f"æ­£åœ¨åˆ†æç¬¬ {i}/{total} ç¯‡: {res.title}")
        code_url = get_code_link(res.entry_id, session=session)
        paper_info = {
            "title": res.title.strip(),
            "summary": (res.summary or "").replace("\n", " ").strip(),
            "url": res.entry_id,
        }

        if args.skip_llm:
            analysis = f"ã€æ‘˜è¦ï¼ˆæœªè°ƒç”¨ LLMï¼‰ã€‘\n{paper_info['summary']}\n"
            score = 3.0
        else:
            try:
                analysis = summarize_with_deepseek(
                    paper_info,
                    prompt_template=prompt_template,
                    api_key=args.deepseek_api_key,
                    api_url=args.deepseek_api_url,
                    model=args.deepseek_model,
                    max_tokens=args.deepseek_max_tokens,
                    session=session,
                )
                # æ‰“å° LLM è¿”å›çš„åŸå§‹å†…å®¹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                print("\n=== LLM è¿”å›å†…å®¹ ===")
                print(analysis)
                print("===================\n")
                score = _extract_score(analysis)
            except Exception as e:
                print(f"LLM è°ƒç”¨å¤±è´¥: {str(e)}")
                analysis = f"ã€LLM è§£æå¤±è´¥ã€‘{str(e)}\n\nã€æ‘˜è¦ã€‘{paper_info['summary']}"
                score = 3.0

        paper_data.append({
            "title": paper_info["title"],
            "url": paper_info["url"],
            "code_url": code_url,
            "analysis": analysis,
            "score": score,
        })

    # ç¬¬äºŒæ­¥ï¼šæŒ‰è¯„åˆ†ä»é«˜åˆ°ä½æ’åº
    paper_data.sort(key=lambda x: x["score"], reverse=True)

    # ç¬¬ä¸‰æ­¥ï¼šè¿‡æ»¤ä½åˆ†è®ºæ–‡ï¼ˆè¯„åˆ† < MIN_SCORE_THRESHOLD çš„ä¸æ¨é€ï¼‰
    filtered_papers = [p for p in paper_data if p["score"] >= MIN_SCORE_THRESHOLD]

    if not filtered_papers:
        msg = f"ä»Šæ—¥æ— é«˜ç›¸å…³æ€§è®ºæ–‡ï¼ˆæ‰€æœ‰è®ºæ–‡è¯„åˆ† < {MIN_SCORE_THRESHOLD}ï¼‰ã€‚"
        print(msg)
        _write_github_step_summary(f"## ArXiv æ¯æ—¥æ¨é€\n\n{msg}\n")
        # ä¸æ¨é€ç©ºæ¶ˆæ¯åˆ°é£ä¹¦
        return 0

    # ç¬¬å››æ­¥ï¼šåªä¿ç•™å‰ FINAL_PUSH_COUNT ç¯‡è®ºæ–‡
    final_papers = filtered_papers[:FINAL_PUSH_COUNT]
    print(f"ç­›é€‰åå…± {len(filtered_papers)} ç¯‡é«˜åˆ†è®ºæ–‡ï¼Œæ¨é€å‰ {len(final_papers)} ç¯‡")

    # ç¬¬äº”æ­¥ï¼šç”Ÿæˆæ¨é€å†…å®¹
    date_label = datetime.now().strftime("%m-%d")
    card_title = f"ğŸš€ ArXiv {date_label}"
    footer_note = f"è‡ªåŠ¨ç”Ÿæˆ | å…± {len(final_papers)} ç¯‡é«˜ç›¸å…³æ€§è®ºæ–‡"

    # ç”Ÿæˆ GitHub Step Summary
    summary_blocks = []
    for i, paper in enumerate(final_papers, start=1):
        code_md = f" | [ğŸ’» ä»£ç ]({paper['code_url']})" if paper.get('code_url') else ""
        header = f"### {i}/{len(final_papers)}. {paper['title']}\nğŸ”— [åŸæ–‡]({paper['url']}){code_md}\n"
        summary_blocks.append(header + paper['analysis'].strip() + "\n")

    summary_md = f"## ArXiv æ¯æ—¥æ¨é€ ({date_label})\n\n" + "\n---\n\n".join(summary_blocks)
    _write_github_step_summary(summary_md)

    if args.dry_run:
        print(summary_md)
        return 0

    # æ¨é€åˆ°é£ä¹¦
    if not args.feishu_webhook:
        print("æœªé…ç½®é£ä¹¦ Webhookï¼Œè·³è¿‡æ¨é€")
        return 0

    if args.per_paper:
        # æ¯ç¯‡è®ºæ–‡å•ç‹¬æ¨é€
        for i, paper in enumerate(final_papers, start=1):
            push_to_feishu(
                [paper],
                webhook=args.feishu_webhook,
                session=session,
                title=f"ğŸš€ ArXiv {date_label} ({i}/{len(final_papers)})",
                footer_note=footer_note,
            )
    else:
        # åˆå¹¶æ¨é€
        push_to_feishu(
            final_papers,
            webhook=args.feishu_webhook,
            session=session,
            title=card_title,
            footer_note=footer_note,
        )

    print("æ¨é€æˆåŠŸï¼")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
