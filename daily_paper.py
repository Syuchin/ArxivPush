import argparse
import json
import os
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta, timezone
from threading import Semaphore
from typing import Any, Optional

import requests

try:
    from dotenv import load_dotenv
except ImportError:  # pragma: no cover
    load_dotenv = None

if load_dotenv is not None:
    load_dotenv()

try:
    from jinja2 import Environment, StrictUndefined
except ImportError:  # pragma: no cover
    Environment = None
    StrictUndefined = None

PWC_BASE_URL = "https://arxiv.paperswithcode.com/api/v0/papers/"
OPENALEX_BASE_URL = "https://api.openalex.org/works"
PUSHED_IDS_FILE = "pushed_ids.txt"  # å­˜å‚¨å·²æ¨é€è®ºæ–‡ ID çš„æ–‡ä»¶

# ============ é…ç½®å¸¸é‡ï¼ˆä¸éšç§çš„é…ç½®ç›´æ¥å†™æ­»ï¼‰ ============

# ArXiv æŸ¥è¯¢å…³é”®è¯ï¼ˆç”¨äº OpenAlex APIï¼‰
# æ‰©å¤§èŒƒå›´ï¼šåŒ…å« LLMã€Agent å’Œå®‰å…¨ç›¸å…³çš„è®ºæ–‡
DEFAULT_ARXIV_QUERY = 'LLM OR agent OR language model OR safety OR security OR robustness'

# æ¯æ¬¡è·å–è®ºæ–‡æ•°é‡ï¼ˆä¼šè·å–æ›´å¤šè®ºæ–‡ï¼Œç„¶åæŒ‰è¯„åˆ†ç­›é€‰ï¼‰
DEFAULT_MAX_RESULTS = 50  # è·å– 50 ç¯‡ï¼Œç­›é€‰å‡ºé«˜åˆ†è®ºæ–‡

# æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰ï¼šè·å–æœ€è¿‘ N å°æ—¶å†…å‘å¸ƒçš„è®ºæ–‡
# è®¾ç½®ä¸º 96 å°æ—¶ï¼ˆ4 å¤©ï¼‰ä»¥è¦†ç›–å‘¨æœ«ï¼ˆArXiv å‘¨æœ«ä¸æ›´æ–°ï¼‰
# é€šè¿‡å»é‡æœºåˆ¶ï¼Œæ¯å¤©åªæ¨é€æ–°è®ºæ–‡ï¼Œå®ç°çœŸæ­£çš„"æ¯æ—¥æ›´æ–°"
DEFAULT_SINCE_HOURS = 96.0

# æœ€ä½è¯„åˆ†é˜ˆå€¼ï¼ˆä½äºæ­¤åˆ†æ•°çš„è®ºæ–‡ä¸æ¨é€ï¼‰
MIN_SCORE_THRESHOLD = 3.0

# 5 åˆ†è®ºæ–‡å…¨éƒ¨æ¨é€ï¼Œå…¶ä»–é«˜åˆ†è®ºæ–‡é™åˆ¶æ•°é‡
PERFECT_SCORE = 5.0  # æ»¡åˆ†è®ºæ–‡
MAX_NON_PERFECT_PAPERS = 20  # éæ»¡åˆ†è®ºæ–‡æœ€å¤šæ¨é€æ•°é‡

# Prompt æ¨¡æ¿æ–‡ä»¶
DEFAULT_PROMPT_FILE = "prompts/deepseek_summary_prompt.zh.j2"

# æ¨¡å‹é…ç½®
DEFAULT_MODEL = "glm-4.7"
DEFAULT_API_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"

# é»˜è®¤ max_tokensï¼ˆGLM-4.7 éœ€è¦æ›´å¤š tokens ç”¨äºæ¨ç†ï¼‰
DEFAULT_MAX_TOKENS = 2000

# å¹¶å‘å¤„ç†çº¿ç¨‹æ•°ï¼ˆä¸¥æ ¼æ§åˆ¶åŒæ—¶è¿›è¡Œçš„ API è¯·æ±‚æ•°ï¼‰
MAX_WORKERS = 3

# é‡è¯•é…ç½®
MAX_RETRIES = 3
RETRY_DELAY = 2  # ç§’

# è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰- é¿å…ç¬é—´å‘é€è¿‡å¤šè¯·æ±‚
REQUEST_INTERVAL = 0.5

# OpenAlex API é…ç½®
OPENALEX_PER_PAGE = 50  # æ¯æ¬¡è¯·æ±‚çš„ç»“æœæ•°é‡
OPENALEX_TIMEOUT = 30  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# é¢„ç¼–è¯‘æ­£åˆ™ï¼šåŒ¹é…ã€ç›¸å…³æ€§ã€‘X/5 æ ¼å¼çš„è¯„åˆ†
_SCORE_RE = re.compile(r'ã€ç›¸å…³æ€§ã€‘\s*(\d+(?:\.\d+)?)\s*/\s*5')

# é¢„ç¼–è¯‘æ­£åˆ™ï¼šåŒ¹é…ã€æ ‡ç­¾åã€‘å†…å®¹ æ ¼å¼çš„åˆ†ææ®µè½
_SECTION_RE = re.compile(r'ã€([^ã€‘]+)ã€‘\s*(.*?)(?=ã€|$)', re.DOTALL)


def _strtobool(v: Optional[str]) -> bool:
    if v is None:
        return False
    return v.strip().lower() in {"1", "true", "t", "yes", "y", "on"}


def _getenv_str(name: str, default: Optional[str] = None) -> Optional[str]:
    raw = os.getenv(name)
    if raw is None:
        return default
    raw = raw.strip()
    if raw == "":
        return default
    return raw


def _getenv_int(name: str, default: int) -> int:
    raw = os.getenv(name)
    if raw is None or raw.strip() == "":
        return default
    return int(raw)


def _getenv_float(name: str, default: float) -> float:
    raw = os.getenv(name)
    if raw is None or raw.strip() == "":
        return default
    return float(raw)


def _resolve_path(path: str) -> str:
    path = os.path.expanduser(path)
    if os.path.isabs(path):
        return path
    base_dir = os.path.dirname(os.path.abspath(__file__))
    return os.path.join(base_dir, path)


def _read_text_file(path: str) -> str:
    resolved = _resolve_path(path)
    with open(resolved, "r", encoding="utf-8") as f:
        return f.read()


def _compile_prompt_template(template_text: str):
    if Environment is None or StrictUndefined is None:  # pragma: no cover
        raise RuntimeError("ç¼ºå°‘ä¾èµ– Jinja2ï¼šè¯·å…ˆå®‰è£… `pip install Jinja2`")
    env = Environment(undefined=StrictUndefined, autoescape=False)
    return env.from_string(template_text)


def get_code_link(arxiv_url: str, session: requests.Session, timeout_s: int = 10) -> Optional[str]:
    """ä» PapersWithCode è·å–ä»£ç é“¾æ¥ï¼ˆè‹¥æœ‰ official repoï¼‰ã€‚"""
    arxiv_id = arxiv_url.rstrip("/").split("/")[-1].split("v")[0]
    try:
        resp = session.get(f"{PWC_BASE_URL}{arxiv_id}", timeout=timeout_s)
        if resp.status_code != 200:
            return None
        data = resp.json()
        official = data.get("official")
        if isinstance(official, dict):
            url = official.get("url")
            if isinstance(url, str) and url.startswith(("http://", "https://")):
                return url
    except requests.RequestException:
        return None
    except ValueError:
        return None
    return None


def _extract_score(analysis: str) -> float:
    """ä»åˆ†ææ–‡æœ¬ä¸­æå–ç›¸å…³æ€§è¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰ï¼Œé»˜è®¤è¿”å› 3.0"""
    # åŒ¹é…ã€ç›¸å…³æ€§ã€‘X/5 æ ¼å¼
    match = re.search(r'ã€ç›¸å…³æ€§ã€‘\s*(\d+(?:\.\d+)?)\s*/\s*5', analysis)
    if match:
        try:
            score = float(match.group(1))
            return max(1.0, min(5.0, score))  # é™åˆ¶åœ¨ 1-5 èŒƒå›´å†…
        except ValueError:
            pass
    return 3.0  # é»˜è®¤ä¸­ç­‰è¯„åˆ†


def _process_single_paper(
    res,
    index: int,
    total: int,
    *,
    session: requests.Session,
    skip_llm: bool,
    prompt_template,
    api_key: str,
    api_url: str,
    model: str,
    max_tokens: int,
    semaphore: Semaphore,
) -> dict:
    """å¤„ç†å•ç¯‡è®ºæ–‡ï¼ˆç”¨äºå¹¶å‘ï¼‰"""
    print(f"æ­£åœ¨åˆ†æç¬¬ {index}/{total} ç¯‡: {res.title}")

    code_url = get_code_link(res.entry_id, session=session)
    paper_info = {
        "title": res.title.strip(),
        "summary": (res.summary or "").replace("\n", " ").strip(),
        "url": res.entry_id,
    }

    if skip_llm:
        analysis = f"ã€æ‘˜è¦ï¼ˆæœªè°ƒç”¨ LLMï¼‰ã€‘\n{paper_info['summary']}\n"
        score = 3.0
    else:
        try:
            # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘ API è¯·æ±‚æ•°
            with semaphore:
                # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…ç¬é—´å‘é€è¿‡å¤šè¯·æ±‚
                time.sleep(REQUEST_INTERVAL)
                analysis = summarize_with_deepseek(
                    paper_info,
                    prompt_template=prompt_template,
                    api_key=api_key,
                    api_url=api_url,
                    model=model,
                    max_tokens=max_tokens,
                    session=session,
                )
            # æ‰“å° LLM è¿”å›çš„åŸå§‹å†…å®¹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            print(f"\n=== è®ºæ–‡ {index} LLM è¿”å›å†…å®¹ ===")
            print(analysis)
            print("===================\n")
            score = _extract_score(analysis)
        except Exception as e:
            print(f"è®ºæ–‡ {index} LLM è°ƒç”¨å¤±è´¥: {str(e)}")
            analysis = f"ã€LLM è§£æå¤±è´¥ã€‘{str(e)}\n\nã€æ‘˜è¦ã€‘{paper_info['summary']}"
            score = 3.0

    return {
        "title": paper_info["title"],
        "url": paper_info["url"],
        "code_url": code_url,
        "analysis": analysis,
        "score": score,
    }


def summarize_with_deepseek(
    paper: dict[str, str],
    *,
    prompt_template,
    api_key: str,
    api_url: str,
    model: str,
    max_tokens: int,
    session: requests.Session,
    timeout_s: int = 120,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 120 ç§’
) -> str:
    """ä½¿ç”¨ DeepSeekï¼ˆOpenAI Chat Completions å…¼å®¹ï¼‰è¿›è¡Œè®ºæ–‡æ·±åº¦æ€»ç»“ã€‚"""
    prompt_text = prompt_template.render(**paper).strip()

    payload: dict[str, Any] = {
        "model": model,
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„å­¦æœ¯è®ºæ–‡ç­›é€‰åŠ©æ‰‹ï¼Œä¸“æ³¨äº LLM Safetyã€Agent Safety å’Œ AI Agent é¢†åŸŸã€‚è¯·å®¢è§‚è¯„ä¼°è®ºæ–‡çš„ç›¸å…³æ€§å’Œåˆ›æ–°æ€§ã€‚",
            },
            {"role": "user", "content": prompt_text},
        ],
        "temperature": 1.0,
        "stream": False,
        "thinking": {
            "type": "disabled"
        },
        "max_tokens": max_tokens,
    }

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }

    # é‡è¯•é€»è¾‘
    for attempt in range(MAX_RETRIES):
        try:
            resp = session.post(api_url, headers=headers, json=payload, timeout=timeout_s)
            resp.raise_for_status()
            res_json = resp.json()

            if isinstance(res_json, dict) and "error" in res_json:
                err = res_json.get("error") or {}
                message = err.get("message") if isinstance(err, dict) else None
                raise RuntimeError(f"DeepSeek API æŠ¥é”™: {message or json.dumps(res_json, ensure_ascii=False)}")

            choices = res_json.get("choices") if isinstance(res_json, dict) else None
            if not isinstance(choices, list) or not choices:
                raise RuntimeError(f"API æœªé¢„æœŸå“åº”: {json.dumps(res_json, ensure_ascii=False)}")

            message = choices[0].get("message") if isinstance(choices[0], dict) else None

            # GLM-4.7 ç­‰æ¨ç†æ¨¡å‹å¯èƒ½å°†å†…å®¹æ”¾åœ¨ reasoning_content ä¸­
            content = None
            if isinstance(message, dict):
                content = message.get("content")
                # å¦‚æœ content ä¸ºç©ºï¼Œå°è¯•ä» reasoning_content ä¸­æå–
                if not content or not content.strip():
                    reasoning_content = message.get("reasoning_content")
                    if reasoning_content:
                        print("è­¦å‘Šï¼šæ¨¡å‹è¿”å›çš„ content ä¸ºç©ºï¼Œä½¿ç”¨ reasoning_content")
                        content = reasoning_content

            if not isinstance(content, str) or not content.strip():
                raise RuntimeError(f"API æœªè¿”å› content: {json.dumps(res_json, ensure_ascii=False)}")

            return content.strip()

        except (requests.exceptions.Timeout, requests.exceptions.HTTPError, Exception) as e:
            is_last_attempt = attempt == MAX_RETRIES - 1

            # 429 é”™è¯¯ä½¿ç”¨æŒ‡æ•°é€€é¿ï¼Œå…¶ä»–é”™è¯¯ä½¿ç”¨çº¿æ€§é€€é¿
            if isinstance(e, requests.exceptions.HTTPError) and e.response.status_code == 429:
                wait_time = RETRY_DELAY * (attempt + 2) * 2
                error_msg = "API é™æµ"
            else:
                wait_time = RETRY_DELAY * (attempt + 1)
                error_msg = "è¯·æ±‚è¶…æ—¶" if isinstance(e, requests.exceptions.Timeout) else f"è¯·æ±‚å¤±è´¥: {str(e)}"

            if not is_last_attempt:
                print(f"{error_msg}ï¼Œ{wait_time} ç§’åé‡è¯•ï¼ˆç¬¬ {attempt + 1}/{MAX_RETRIES} æ¬¡ï¼‰...")
                time.sleep(wait_time)
            else:
                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                raise


def _extract_section(analysis: str, section_name: str) -> Optional[str]:
    """ä»åˆ†ææ–‡æœ¬ä¸­æå–æŒ‡å®šæ®µè½çš„å†…å®¹"""
    match = re.search(rf'ã€{re.escape(section_name)}ã€‘\s*(.*?)(?=ã€|$)', analysis, re.DOTALL)
    return match.group(1).strip() if match else None


def _feishu_card_payload(title: str, papers: list[dict], footer_note: str) -> dict[str, Any]:
    """ç”Ÿæˆé£ä¹¦å¯Œæ–‡æœ¬å¡ç‰‡ payloadï¼ˆæ”¯æŒå¤šç¯‡è®ºæ–‡ï¼‰"""
    elements = []

    for i, paper in enumerate(papers):
        analysis = paper['analysis']

        # æå–è¯„åˆ†
        score_match = re.search(r'ã€ç›¸å…³æ€§ã€‘\s*(\d+(?:\.\d+)?)\s*/\s*5', analysis)
        score_text = f"<font color='red'>({score_match.group(1)}/5)</font>" if score_match else ""

        # æ ‡é¢˜ï¼ˆå¸¦è¯„åˆ†ï¼‰
        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"**{i+1}/{len(papers)}. <font color='blue'>{paper['title']}</font>** {score_text}"
            }
        })

        # é“¾æ¥æŒ‰é’®
        actions = [{
            "tag": "button",
            "text": {"tag": "plain_text", "content": "æŸ¥çœ‹è®ºæ–‡"},
            "type": "primary",
            "url": paper['url']
        }]
        if paper.get('code_url'):
            actions.append({
                "tag": "button",
                "text": {"tag": "plain_text", "content": "æŸ¥çœ‹ä»£ç "},
                "type": "default",
                "url": paper['code_url']
            })
        elements.append({"tag": "action", "actions": actions})

        # åˆå¹¶ï¼šé—®é¢˜å®šä¹‰ + æ–¹æ³•æ ¸å¿ƒ + ä¸»è¦å‘ç°
        core_sections = [
            ("é—®é¢˜å®šä¹‰", "violet"),
            ("æ–¹æ³•æ ¸å¿ƒ", "blue"),
            ("ä¸»è¦å‘ç°", "violet"),
        ]
        core_content = []
        for section_name, color in core_sections:
            content = _extract_section(analysis, section_name)
            if content:
                core_content.append(f"<font color='{color}'>**ã€{section_name}ã€‘**</font>\n{content}")

        if core_content:
            elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": "\n".join(core_content)
                }
            })

        # åˆå¹¶ï¼šå±€é™æ€§æ¨æµ‹ + æ½œåœ¨å…³è”
        analysis_sections = [
            ("å±€é™æ€§æ¨æµ‹", "orange"),
            ("æ½œåœ¨å…³è”", "green"),
        ]
        analysis_content = []
        for section_name, color in analysis_sections:
            content = _extract_section(analysis, section_name)
            if content:
                analysis_content.append(f"<font color='{color}'>**ã€{section_name}ã€‘**</font>\n{content}")

        if analysis_content:
            elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": "\n".join(analysis_content)
                }
            })

        # æå–ä¸€å¥è¯ç»“è®º
        conclusion = _extract_section(analysis, "ä¸€å¥è¯ç»“è®º")
        if conclusion:
            elements.append({
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"<font color='blue'>**ã€ä¸€å¥è¯ç»“è®ºã€‘**</font>\n{conclusion}"
                }
            })

        # å¦‚æœä¸æ˜¯æœ€åä¸€ç¯‡ï¼Œæ·»åŠ åˆ†éš”çº¿
        if i < len(papers) - 1:
            elements.append({"tag": "hr"})

    # æ·»åŠ é¡µè„š
    elements.append({"tag": "hr"})
    elements.append({
        "tag": "note",
        "elements": [{"tag": "plain_text", "content": footer_note}]
    })

    return {
        "msg_type": "interactive",
        "card": {
            "header": {
                "title": {"tag": "plain_text", "content": title},
                "template": "blue"
            },
            "elements": elements
        }
    }


def push_to_feishu(
    papers: list[dict],
    *,
    webhook: str,
    session: requests.Session,
    title: str,
    footer_note: str,
    timeout_s: int = 15,
) -> None:
    """å‘é€é£ä¹¦å¯Œæ–‡æœ¬å¡ç‰‡ï¼ˆå¤±è´¥ä¼šæŠ›å¼‚å¸¸ï¼‰ã€‚

    Args:
        papers: è®ºæ–‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« title, url, code_url, analysis
        webhook: é£ä¹¦ Webhook åœ°å€
        session: requests.Session å¯¹è±¡
        title: å¡ç‰‡æ ‡é¢˜
        footer_note: é¡µè„šæ–‡æœ¬
        timeout_s: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    """
    headers = {"Content-Type": "application/json"}
    payload = _feishu_card_payload(title=title, papers=papers, footer_note=footer_note)
    resp = session.post(webhook, headers=headers, json=payload, timeout=timeout_s)
    resp.raise_for_status()
    data = resp.json()
    if isinstance(data, dict):
        code = data.get("code", data.get("StatusCode"))
        if code not in (0, "0", None):
            raise RuntimeError(f"é£ä¹¦è¿”å›é”™è¯¯: {json.dumps(data, ensure_ascii=False)}")




def fetch_papers_from_openalex(
    query: str,
    max_results: int,
    since_hours: float,
    email: str,
    session: requests.Session,
    api_key: Optional[str] = None,
) -> list[dict]:
    """
    ä» OpenAlex API è·å– ArXiv è®ºæ–‡

    Args:
        query: æœç´¢å…³é”®è¯
        max_results: æœ€å¤§ç»“æœæ•°
        since_hours: åªè·å–æœ€è¿‘ N å°æ—¶å†…çš„è®ºæ–‡ï¼ˆ0 è¡¨ç¤ºä¸é™åˆ¶ï¼‰
        email: ç”¨äº Polite Pool çš„é‚®ç®±åœ°å€
        session: requests.Session å¯¹è±¡
        api_key: OpenAlex API Keyï¼ˆå¯é€‰ï¼Œç”¨äºè®¿é—®é«˜çº§åŠŸèƒ½å¦‚ from_created_dateï¼‰

    Returns:
        è®ºæ–‡åˆ—è¡¨ï¼Œæ¯ä¸ªè®ºæ–‡æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« title, summary, entry_id, published ç­‰å­—æ®µ
    """
    print(f"æ­£åœ¨ä» OpenAlex API æŸ¥è¯¢è®ºæ–‡ï¼ˆå…³é”®è¯ï¼š{query}ï¼‰...")

    # æ„å»ºæŸ¥è¯¢å‚æ•°
    # å¦‚æœæœ‰ API Keyï¼Œä½¿ç”¨ from_created_dateï¼ˆæ›´å‡†ç¡®ï¼‰
    # å¦åˆ™ä½¿ç”¨ from_publication_date
    use_created_date = api_key is not None
    date_filter_type = 'from_created_date' if use_created_date else 'from_publication_date'
    sort_field = 'created_date' if use_created_date else 'publication_date'

    params = {
        'filter': f'indexed_in:arxiv,title.search:{query}',
        'sort': f'{sort_field}:desc',
        'mailto': email,
        'per_page': min(OPENALEX_PER_PAGE, max_results),
    }

    # å¦‚æœæŒ‡å®šäº†æ—¶é—´èŒƒå›´ï¼Œæ·»åŠ æ—¥æœŸè¿‡æ»¤
    if since_hours > 0:
        now = datetime.now(timezone.utc)
        threshold = now - timedelta(hours=since_hours)
        # OpenAlex ä½¿ç”¨ YYYY-MM-DD æ ¼å¼
        from_date = threshold.strftime('%Y-%m-%d')
        params['filter'] += f',{date_filter_type}:{from_date}'
        if use_created_date:
            print(f"è¿‡æ»¤æ¡ä»¶ï¼šåªè·å– {from_date} ä¹‹åæ·»åŠ åˆ° OpenAlex çš„è®ºæ–‡ï¼ˆä½¿ç”¨ API Keyï¼‰")
        else:
            print(f"è¿‡æ»¤æ¡ä»¶ï¼šåªè·å– {from_date} ä¹‹åå‘å¸ƒçš„è®ºæ–‡")

    papers = []
    page = 1

    while len(papers) < max_results:
        try:
            params['page'] = page

            # å¦‚æœæœ‰ API Keyï¼Œæ·»åŠ åˆ°è¯·æ±‚å¤´
            headers = {}
            if api_key:
                headers['Authorization'] = f'Bearer {api_key}'

            response = session.get(
                OPENALEX_BASE_URL,
                params=params,
                headers=headers,
                timeout=OPENALEX_TIMEOUT
            )
            response.raise_for_status()
            data = response.json()

            results = data.get('results', [])
            if not results:
                break

            for work in results:
                if len(papers) >= max_results:
                    break

                # æå– ArXiv ID å’Œ URL
                arxiv_id = None
                arxiv_url = None

                # ä» ids å­—æ®µä¸­æå– ArXiv ID
                ids = work.get('ids', {})
                if 'arxiv' in ids:
                    arxiv_url = ids['arxiv']
                    # ä» URL ä¸­æå– ID
                    if arxiv_url:
                        match = re.search(r'arxiv\.org/abs/(\S+)', arxiv_url)
                        if match:
                            arxiv_id = match.group(1)

                # å¦‚æœæ²¡æœ‰æ‰¾åˆ° ArXiv URLï¼Œå°è¯•ä» locations ä¸­æŸ¥æ‰¾
                if not arxiv_url:
                    locations = work.get('locations', [])
                    for loc in locations:
                        landing_page = loc.get('landing_page_url', '')
                        if 'arxiv.org' in landing_page:
                            arxiv_url = landing_page
                            match = re.search(r'arxiv\.org/abs/(\S+)', landing_page)
                            if match:
                                arxiv_id = match.group(1)
                            break

                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œè·³è¿‡è¿™ç¯‡è®ºæ–‡
                if not arxiv_url:
                    continue

                # æå–æ‘˜è¦
                abstract = work.get('abstract', '')
                if not abstract:
                    # å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œå°è¯•ä½¿ç”¨ abstract_inverted_index
                    inv_index = work.get('abstract_inverted_index', {})
                    if inv_index:
                        # é‡å»ºæ‘˜è¦æ–‡æœ¬
                        words = []
                        for word, positions in inv_index.items():
                            for pos in positions:
                                words.append((pos, word))
                        words.sort()
                        abstract = ' '.join([w[1] for w in words])

                # æå–å‘å¸ƒæ—¥æœŸ
                pub_date_str = work.get('publication_date')
                published = None
                if pub_date_str:
                    try:
                        published = datetime.strptime(pub_date_str, '%Y-%m-%d').replace(tzinfo=timezone.utc)
                    except ValueError:
                        pass

                # æå– OpenAlex IDï¼ˆç”¨äºå»é‡ï¼‰
                openalex_id = work.get('id', '')

                # æ„å»ºä¸ ArXiv API å…¼å®¹çš„è®ºæ–‡å¯¹è±¡ï¼ˆä½¿ç”¨ç®€å•çš„å­—å…¸æ¨¡æ‹Ÿ arxiv.Resultï¼‰
                class PaperResult:
                    def __init__(self, title, summary, entry_id, published, openalex_id):
                        self.title = title
                        self.summary = summary
                        self.entry_id = entry_id
                        self.published = published
                        self.openalex_id = openalex_id  # ç”¨äºå»é‡

                paper = PaperResult(
                    title=work.get('display_name', '').strip(),
                    summary=abstract.strip() if abstract else '',
                    entry_id=arxiv_url,
                    published=published,
                    openalex_id=openalex_id,
                )

                papers.append(paper)

            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šç»“æœ
            meta = data.get('meta', {})
            if page * params['per_page'] >= meta.get('count', 0):
                break

            page += 1
            time.sleep(REQUEST_INTERVAL)  # é¿å…è¯·æ±‚è¿‡å¿«

        except requests.exceptions.RequestException as e:
            print(f"OpenAlex API è¯·æ±‚å¤±è´¥: {str(e)}", file=sys.stderr)
            break
        except Exception as e:
            print(f"è§£æ OpenAlex å“åº”æ—¶å‡ºé”™: {str(e)}", file=sys.stderr)
            break

    print(f"ä» OpenAlex è·å–åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    return papers


def _write_github_step_summary(markdown: str) -> None:
    path = os.getenv("GITHUB_STEP_SUMMARY")
    if not path:
        return
    try:
        with open(path, "a", encoding="utf-8") as f:
            f.write(markdown.rstrip() + "\n")
    except OSError:
        pass


def load_pushed_ids() -> set[str]:
    """åŠ è½½å·²æ¨é€çš„è®ºæ–‡ ID åˆ—è¡¨"""
    if not os.path.exists(PUSHED_IDS_FILE):
        return set()
    try:
        with open(PUSHED_IDS_FILE, 'r', encoding='utf-8') as f:
            # åªä¿ç•™æœ€è¿‘ 500 æ¡è®°å½•ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§
            ids = [line.strip() for line in f if line.strip()]
            return set(ids[-500:])
    except Exception as e:
        print(f"è¯»å–å·²æ¨é€ ID æ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)
        return set()


def save_pushed_ids(ids: set[str]) -> None:
    """ä¿å­˜å·²æ¨é€çš„è®ºæ–‡ ID åˆ—è¡¨"""
    try:
        # åªä¿ç•™æœ€è¿‘ 500 æ¡è®°å½•
        ids_list = list(ids)[-500:]
        with open(PUSHED_IDS_FILE, 'w', encoding='utf-8') as f:
            f.write('\n'.join(ids_list))
        print(f"å·²æ›´æ–°æ¨é€è®°å½•æ–‡ä»¶ï¼Œå½“å‰è®°å½•æ•°: {len(ids_list)}")
    except Exception as e:
        print(f"ä¿å­˜å·²æ¨é€ ID æ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)


def _write_github_step_summary(markdown: str) -> None:
    path = os.getenv("GITHUB_STEP_SUMMARY")
    if not path:
        return
    try:
        with open(path, "a", encoding="utf-8") as f:
            f.write(markdown.rstrip() + "\n")
    except OSError:
        return


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Fetch latest arXiv papers, summarize, and push to Feishu.")
    parser.add_argument("--query", default=_getenv_str("ARXIV_QUERY", DEFAULT_ARXIV_QUERY))
    parser.add_argument("--max-results", type=int, default=_getenv_int("MAX_RESULTS", DEFAULT_MAX_RESULTS))
    parser.add_argument("--since-hours", type=float, default=_getenv_float("SINCE_HOURS", DEFAULT_SINCE_HOURS))

    # OpenAlex API é…ç½®
    parser.add_argument("--openalex-email", default=_getenv_str("OPENALEX_EMAIL"), help="Email for OpenAlex Polite Pool")
    parser.add_argument("--openalex-api-key", default=_getenv_str("OPENALEX_API_KEY"), help="OpenAlex API Key (optional, for advanced features)")

    parser.add_argument("--feishu-webhook", default=_getenv_str("FEISHU_WEBHOOK"))
    parser.add_argument("--per-paper", action="store_true", default=_strtobool(os.getenv("FEISHU_PER_PAPER")))

    parser.add_argument("--deepseek-api-key", default=_getenv_str("DEEPSEEK_API_KEY"))
    parser.add_argument("--deepseek-model", default=_getenv_str("DEEPSEEK_MODEL", DEFAULT_MODEL))
    parser.add_argument("--deepseek-api-url", default=_getenv_str("DEEPSEEK_API_URL"))  # å¦‚æœæœªæŒ‡å®šï¼Œå°†æ ¹æ®æ¨¡å‹è‡ªåŠ¨é€‰æ‹©
    parser.add_argument("--deepseek-max-tokens", type=int, default=_getenv_int("DEEPSEEK_MAX_TOKENS", DEFAULT_MAX_TOKENS))
    parser.add_argument("--skip-llm", action="store_true", default=_strtobool(os.getenv("SKIP_LLM")))
    parser.add_argument("--prompt-file", default=_getenv_str("PROMPT_FILE", DEFAULT_PROMPT_FILE))

    parser.add_argument("--dry-run", action="store_true", default=_strtobool(os.getenv("DRY_RUN")))
    return parser.parse_args()


def main() -> int:
    args = _parse_args()
    session = requests.Session()

    # å¦‚æœæœªæŒ‡å®š API URLï¼Œä½¿ç”¨é»˜è®¤å€¼
    if not args.deepseek_api_url:
        args.deepseek_api_url = DEFAULT_API_URL
        print(f"ä½¿ç”¨é»˜è®¤ API ç«¯ç‚¹ï¼š{args.deepseek_api_url}ï¼ˆæ¨¡å‹ï¼š{args.deepseek_model}ï¼‰")

    if not args.dry_run and not args.feishu_webhook:
        print("ç¼ºå°‘ FEISHU_WEBHOOKï¼šè¯·åœ¨ç¯å¢ƒå˜é‡æˆ–å‚æ•°ä¸­è®¾ç½® --feishu-webhookã€‚", file=sys.stderr)
        return 2
    if not args.skip_llm and not args.deepseek_api_key:
        print("ç¼ºå°‘ DEEPSEEK_API_KEYï¼šè¯·åœ¨ç¯å¢ƒå˜é‡æˆ–å‚æ•°ä¸­è®¾ç½® --deepseek-api-keyï¼Œæˆ–ä½¿ç”¨ --skip-llmã€‚", file=sys.stderr)
        return 2
    if not args.openalex_email:
        print("ç¼ºå°‘ OPENALEX_EMAILï¼šè¯·åœ¨ç¯å¢ƒå˜é‡æˆ–å‚æ•°ä¸­è®¾ç½® --openalex-emailã€‚", file=sys.stderr)
        return 2

    prompt_template = None
    if not args.skip_llm:
        try:
            template_text = _read_text_file(args.prompt_file)
            prompt_template = _compile_prompt_template(template_text)
        except Exception as e:
            print(f"æ— æ³•åŠ è½½ Prompt æ¨¡æ¿ï¼ˆ{args.prompt_file}ï¼‰ï¼š{e}", file=sys.stderr)
            return 2

    print("æ­£åœ¨æœé›†æœ€æ–°è®ºæ–‡...")
    # ä½¿ç”¨ OpenAlex API è·å–è®ºæ–‡
    results = fetch_papers_from_openalex(
        query=args.query,
        max_results=args.max_results,
        since_hours=args.since_hours,
        email=args.openalex_email,
        session=session,
        api_key=args.openalex_api_key,
    )

    if not results:
        msg = "ä»Šæ—¥æš‚æ— æ–°è®ºæ–‡ã€‚"
        print(msg)
        _write_github_step_summary(f"## ArXiv æ¯æ—¥æ¨é€\n\n{msg}\n")
        if not args.dry_run and args.feishu_webhook:
            push_to_feishu(
                msg,
                webhook=args.feishu_webhook,
                session=session,
                title=f"ğŸš€ ArXiv {datetime.now().strftime('%m-%d')}",
                footer_note="è‡ªåŠ¨ç”Ÿæˆï¼šæ— æ–°è®ºæ–‡",
            )
        return 0

    # åŠ è½½å·²æ¨é€çš„è®ºæ–‡ ID å¹¶è¿›è¡Œå»é‡
    pushed_ids = load_pushed_ids()
    original_count = len(results)

    # è¿‡æ»¤æ‰å·²æ¨é€çš„è®ºæ–‡
    results = [r for r in results if getattr(r, 'openalex_id', '') not in pushed_ids]

    if len(results) < original_count:
        print(f"å»é‡ï¼šè¿‡æ»¤æ‰ {original_count - len(results)} ç¯‡å·²æ¨é€çš„è®ºæ–‡ï¼Œå‰©ä½™ {len(results)} ç¯‡")

    if not results:
        msg = "ä»Šæ—¥æ— æ–°è®ºæ–‡ï¼ˆæ‰€æœ‰è®ºæ–‡å‡å·²æ¨é€è¿‡ï¼‰ã€‚"
        print(msg)
        _write_github_step_summary(f"## ArXiv æ¯æ—¥æ¨é€\n\n{msg}\n")
        if not args.dry_run and args.feishu_webhook:
            push_to_feishu(
                msg,
                webhook=args.feishu_webhook,
                session=session,
                title=f"ğŸš€ ArXiv {datetime.now().strftime('%m-%d')}",
                footer_note="è‡ªåŠ¨ç”Ÿæˆï¼šæ— æ–°è®ºæ–‡",
            )
        return 0

    # ç¬¬ä¸€æ­¥ï¼šå¹¶å‘åˆ†ææ‰€æœ‰è®ºæ–‡å¹¶æå–è¯„åˆ†
    paper_data: list[dict] = []
    total = len(results)

    print(f"å¼€å§‹å¹¶å‘åˆ†æ {total} ç¯‡è®ºæ–‡ï¼ˆå¹¶å‘æ•°ï¼š{MAX_WORKERS}ï¼‰...")

    # åˆ›å»ºä¿¡å·é‡ï¼Œä¸¥æ ¼æ§åˆ¶åŒæ—¶è¿›è¡Œçš„ API è¯·æ±‚æ•°
    api_semaphore = Semaphore(MAX_WORKERS)

    # ThreadPoolExecutor çš„ max_workers è®¾ä¸º MAX_WORKERS å³å¯
    # Semaphore å·²ç»æ§åˆ¶äº†å¹¶å‘ API è¯·æ±‚æ•°ï¼Œä¸éœ€è¦é¢å¤–çš„çº¿ç¨‹
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_index = {
            executor.submit(
                _process_single_paper,
                res,
                i,
                total,
                session=session,
                skip_llm=args.skip_llm,
                prompt_template=prompt_template,
                api_key=args.deepseek_api_key,
                api_url=args.deepseek_api_url,
                model=args.deepseek_model,
                max_tokens=args.deepseek_max_tokens,
                semaphore=api_semaphore,
            ): i
            for i, res in enumerate(results, start=1)
        }

        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_index):
            try:
                paper = future.result()
                paper_data.append(paper)
            except Exception as e:
                index = future_to_index[future]
                print(f"å¤„ç†è®ºæ–‡ {index} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                # ç»§ç»­å¤„ç†å…¶ä»–è®ºæ–‡

    # ç¬¬äºŒæ­¥ï¼šæŒ‰è¯„åˆ†ä»é«˜åˆ°ä½æ’åº
    paper_data.sort(key=lambda x: x["score"], reverse=True)

    # ç¬¬ä¸‰æ­¥ï¼šè¿‡æ»¤ä½åˆ†è®ºæ–‡ï¼ˆè¯„åˆ† < MIN_SCORE_THRESHOLD çš„ä¸æ¨é€ï¼‰
    filtered_papers = [p for p in paper_data if p["score"] >= MIN_SCORE_THRESHOLD]

    if not filtered_papers:
        msg = f"ä»Šæ—¥æ— é«˜ç›¸å…³æ€§è®ºæ–‡ï¼ˆæ‰€æœ‰è®ºæ–‡è¯„åˆ† < {MIN_SCORE_THRESHOLD}ï¼‰ã€‚"
        print(msg)
        _write_github_step_summary(f"## ArXiv æ¯æ—¥æ¨é€\n\n{msg}\n")
        # ä¸æ¨é€ç©ºæ¶ˆæ¯åˆ°é£ä¹¦
        return 0

    # ç¬¬å››æ­¥ï¼šåˆ†ç¦»æ»¡åˆ†è®ºæ–‡å’Œå…¶ä»–é«˜åˆ†è®ºæ–‡
    # æ»¡åˆ†è®ºæ–‡ï¼ˆ5 åˆ†ï¼‰å…¨éƒ¨æ¨é€ï¼Œå…¶ä»–é«˜åˆ†è®ºæ–‡æœ€å¤šæ¨é€ MAX_NON_PERFECT_PAPERS ç¯‡
    perfect_papers = [p for p in filtered_papers if p["score"] >= PERFECT_SCORE]
    other_papers = [p for p in filtered_papers if p["score"] < PERFECT_SCORE]

    # ç»„åˆï¼šæ‰€æœ‰æ»¡åˆ†è®ºæ–‡ + æœ€å¤š MAX_NON_PERFECT_PAPERS ç¯‡å…¶ä»–é«˜åˆ†è®ºæ–‡
    final_papers = perfect_papers + other_papers[:MAX_NON_PERFECT_PAPERS]

    print(f"ç­›é€‰åå…± {len(filtered_papers)} ç¯‡é«˜åˆ†è®ºæ–‡ï¼š")
    print(f"  - æ»¡åˆ†è®ºæ–‡ï¼ˆ5 åˆ†ï¼‰ï¼š{len(perfect_papers)} ç¯‡ï¼ˆå…¨éƒ¨æ¨é€ï¼‰")
    print(f"  - å…¶ä»–é«˜åˆ†è®ºæ–‡ï¼ˆ3-4.9 åˆ†ï¼‰ï¼š{len(other_papers)} ç¯‡ï¼ˆæ¨é€å‰ {min(len(other_papers), MAX_NON_PERFECT_PAPERS)} ç¯‡ï¼‰")
    print(f"  - æœ€ç»ˆæ¨é€ï¼š{len(final_papers)} ç¯‡")

    # ç¬¬äº”æ­¥ï¼šç”Ÿæˆæ¨é€å†…å®¹
    date_label = datetime.now().strftime("%m-%d")
    card_title = f"ğŸš€ ArXiv {date_label}"
    footer_note = f"è‡ªåŠ¨ç”Ÿæˆ | å…± {len(final_papers)} ç¯‡é«˜ç›¸å…³æ€§è®ºæ–‡"

    # ç”Ÿæˆ GitHub Step Summary
    summary_blocks = []
    for i, paper in enumerate(final_papers, start=1):
        code_md = f" | [ğŸ’» ä»£ç ]({paper['code_url']})" if paper.get('code_url') else ""
        header = f"### {i}/{len(final_papers)}. {paper['title']}\nğŸ”— [åŸæ–‡]({paper['url']}){code_md}\n"
        summary_blocks.append(header + paper['analysis'].strip() + "\n")

    summary_md = f"## ArXiv æ¯æ—¥æ¨é€ ({date_label})\n\n" + "\n---\n\n".join(summary_blocks)
    _write_github_step_summary(summary_md)

    if args.dry_run:
        print(summary_md)
        return 0

    # æ¨é€åˆ°é£ä¹¦
    if not args.feishu_webhook:
        print("æœªé…ç½®é£ä¹¦ Webhookï¼Œè·³è¿‡æ¨é€")
        return 0

    if args.per_paper:
        # æ¯ç¯‡è®ºæ–‡å•ç‹¬æ¨é€
        for i, paper in enumerate(final_papers, start=1):
            push_to_feishu(
                [paper],
                webhook=args.feishu_webhook,
                session=session,
                title=f"ğŸš€ ArXiv {date_label} ({i}/{len(final_papers)})",
                footer_note=footer_note,
            )
    else:
        # åˆå¹¶æ¨é€
        push_to_feishu(
            final_papers,
            webhook=args.feishu_webhook,
            session=session,
            title=card_title,
            footer_note=footer_note,
        )

    # ä¿å­˜å·²æ¨é€çš„è®ºæ–‡ ID
    # ä» results ä¸­æå–æ‰€æœ‰è¢«å¤„ç†çš„è®ºæ–‡çš„ OpenAlex ID
    processed_ids = {getattr(r, 'openalex_id', '') for r in results if getattr(r, 'openalex_id', '')}
    if processed_ids:
        pushed_ids.update(processed_ids)
        save_pushed_ids(pushed_ids)

    print("æ¨é€æˆåŠŸï¼")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
